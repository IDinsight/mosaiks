{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no shapefiles for SHRUG...\n",
    "\n",
    "\"The SHRUG does not include geographic data in the form of polygons or shapefiles because\n",
    "we have not yet found a sufficiently accurate data source with open sharing privileges. We are\n",
    "continuing to investigate sources of geographic data and may include shapefiles in a future version\n",
    "of the SHRUG. Users interested in obtaining geocodes or polygons for SHRUG units are advised\n",
    "to examine the open village maps offered by NASA-SEDAC at Columbia University. These can\n",
    "be directly merged to the 2001 Population Census SHRUG keys in shrug pc01r key.dta and\n",
    "shrug pc01u key.dta. Our own aggregate data was based on 2011 village polygons which we\n",
    "believe are slightly more accurate but are not made available with an open data license.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load shapefiles (NASA-SEDAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_dbf = gpd.read_file(\n",
    "    f\"../data/NASA/india-india-village-level-geospatial-socio-econ-1991-2001-ap-2001-shp/india-village-census-2001-AP.dbf\"\n",
    ")\n",
    "# convert to Lat-Long coords\n",
    "ap_dbf = ap_dbf.to_crs(epsg=4326)\n",
    "ap_dbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_dbf.plot(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unique ID columns to match SHRUG keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Number of towns with no village code\n",
    "ap_dbf.dropna(subset=[\"TOWN_VILL\"])[\"VILL_CODE\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that are missing any identifiers \n",
    "ap_dbf_clean = ap_dbf.dropna(subset=[\"SID\", \"DID\", \"TID\", \"VILL_CODE\"]).copy()\n",
    "\n",
    "# remove the leading 0s\n",
    "for var in [\"SID\", \"DID\", \"TID\", \"VILL_CODE\"]:\n",
    "    ap_dbf_clean[var] = ap_dbf_clean[var].str.lstrip(\"0\")\n",
    "\n",
    "# create combined ID column\n",
    "ap_dbf_clean[\"ID\"] = (\n",
    "    ap_dbf_clean[\"SID\"] + \"-\"\n",
    "    + ap_dbf_clean[\"DID\"] + \"-\"\n",
    "    + ap_dbf_clean[\"TID\"] + \"-\"\n",
    "    + ap_dbf_clean[\"VILL_CODE\"]\n",
    ")\n",
    "ap_dbf_clean.sort_values(by=[\"ID\"], inplace=True)\n",
    "\n",
    "print(\"Number of unique IDs: \", ap_dbf_clean[\"ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_dbf_clean.plot(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SHRUG keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RURAL\n",
    "shrug_pc01r_key = pd.read_csv(\n",
    "    \"../data/SHRUG/shrug-v1.5.samosa-keys-csv/shrug_pc01r_key.csv\"\n",
    ")\n",
    "shrug_pc01r_key.head()\n",
    "\n",
    "### URBAN\n",
    "# shrug_pc01u_key = pd.read_csv(\n",
    "#     \"../data/SHRUG/shrug-v1.5.samosa-keys-csv/shrug_pc01u_key.csv\"\n",
    "# )\n",
    "# shrug_pc01u_key.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ID column to match IDs in NASA-SEDAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrug_pc01r_key_clean = shrug_pc01r_key.dropna(subset=[\"pc01_state_id\", \"pc01_district_id\", \"pc01_subdistrict_id\", \"pc01_village_id\"]).copy()\n",
    "\n",
    "shrug_pc01r_key_clean[\"ID\"] = (\n",
    "    shrug_pc01r_key_clean[\"pc01_state_id\"].astype(int).astype(str) + \"-\"\n",
    "    + shrug_pc01r_key_clean[\"pc01_district_id\"].astype(int).astype(str) + \"-\"\n",
    "    + shrug_pc01r_key_clean[\"pc01_subdistrict_id\"].astype(int).astype(str) + \"-\"\n",
    "    + shrug_pc01r_key_clean[\"pc01_village_id\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match shapes to SHRUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrid_geom_df = pd.merge(ap_dbf_clean[[\"geometry\", \"ID\"]], shrug_pc01r_key_clean, on=\"ID\", how=\"inner\")\n",
    "shrid_geom_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: multiple villages can be inside the same shrid ID - we can merge the shape of these villages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "shrid_geom_df[shrid_geom_df[\"shrid\"]==\"11-28-803020\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrid_geom_df = shrid_geom_df.dissolve(by='shrid', aggfunc='sum').reset_index()\n",
    "# Note: \"ID\" columns gets dropped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrid_geom_df.plot(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of coords to fetch from MOSAIKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = shrid_geom_df.bounds\n",
    "\n",
    "min_long = bounds[\"minx\"].min().round(2)\n",
    "min_lat = bounds[\"miny\"].min().round(2)\n",
    "max_long = bounds[\"maxx\"].max().round(2)\n",
    "max_lat = bounds[\"maxy\"].max().round(2)\n",
    "\n",
    "print(\"LAT min:\", min_lat, \"max:\", max_lat)\n",
    "print(\"LONG min:\", min_long, \"max:\", max_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list containing grid coords\n",
    "lat_list = np.arange(min_lat, max_lat, 0.05).round(2)\n",
    "long_list = np.arange(min_long, max_long, 0.05).round(2)\n",
    "\n",
    "coords_list = np.array([(lat, long) for lat in lat_list for long in long_list]).round(2)\n",
    "print(\"Number of coord pairs in grid:\", len(coords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert coords pairs to geopandas points\n",
    "geometry = gpd.points_from_xy(x=coords_list[:, 1], y=coords_list[:, 0])\n",
    "coords_gdf = gpd.GeoDataFrame(geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Filter coords list to only include points that land within the areas of the shapefile\n",
    "selected_coords_gdf = coords_gdf.sjoin(shrid_geom_df, how=\"inner\")\n",
    "selected_coords_gdf = selected_coords_gdf.drop(\"index_right\", axis=1).sort_values(by=[\"shrid\"])\n",
    "selected_coords_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_coords_gdf.plot(column=selected_coords_gdf[\"pc01_subdistrict_id\"].astype(str), figsize=(10, 10), markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export list of coordinates...\n",
    "\n",
    "...to download MOSAIKS features for through File Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_coords = pd.DataFrame({\"Latitude\":selected_coords_gdf.geometry.y, \"Longitude\":selected_coords_gdf.geometry.x})\n",
    "selected_coords.to_csv(\"../data/MOSAIKS/coords_request_AP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MOSAIKS (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load resulting MOSAIKS features download..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaiks_features = pd.read_csv(\"../data/MOSAIKS/Mosaiks_features.csv\")\n",
    "mosaiks_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make into a GeoDataFrame\n",
    "mosaiks_coords_points = gpd.points_from_xy(x=mosaiks_features[\"Lon\"], y=mosaiks_features[\"Lat\"])\n",
    "mosaiks_features_gdf = gpd.GeoDataFrame(mosaiks_features, geometry=mosaiks_coords_points, crs=\"EPSG:4326\")\n",
    "\n",
    "mosaiks_features_gdf.plot(figsize=(10, 10), markersize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add shrid column to mosaiks features, based on whether the the MOSAIKS coordinate is within the shrid area\n",
    "mosaiks_features_df = mosaiks_features_gdf.sjoin(shrid_geom_df)\n",
    "mosaiks_features_df.drop(columns=[\"index_right\"], inplace=True)\n",
    "mosaiks_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SHRUG SECC (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrug_secc = pd.read_csv(\n",
    "    \"../data/SHRUG/shrug-v1.5.samosa-secc-csv/shrug_secc.csv\"\n",
    ")\n",
    "shrug_secc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entries with no rural pov rate\n",
    "shrug_secc_pov_r = shrug_secc[[\"shrid\", \"secc_pov_rate_tend_rural\"]].copy()\n",
    "shrug_secc_pov_r.dropna(inplace=True)\n",
    "shrug_secc_pov_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match target to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MOSAIKS features to the SECC data\n",
    "df = pd.merge(shrug_secc_pov_r, mosaiks_features_df, on=\"shrid\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 4:-6]\n",
    "# X = X.sample(1000, axis=1, random_state=42)\n",
    "X.insert(0, \"Lat\", df[\"Lat\"])\n",
    "X.insert(0, \"Lon\", df[\"Lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"secc_pov_rate_tend_rural\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling borrowed from MOSAIKS Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a ridge regression of label on the MOSAIKS features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step learns how the visual information in the imagery, as captured by the features, relates to the labels. \n",
    "\n",
    "Before performing a ridge regression, we first split our data into train (80%) and test (20%). We will estimate the models on the train set and then evaluate predictions in the test set. This separation of train and test set is important to address issues related to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1,)\n",
    "\n",
    "#save test set lat/lons for later\n",
    "plotting_coords = X_test.loc[:, [\"Lat\", \"Lon\"]]\n",
    "\n",
    "#remove lat/lons columns\n",
    "X_train = X_train.drop(columns=[\"Lat\", \"Lon\"])\n",
    "X_test = X_test.drop(columns=[\"Lat\", \"Lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "# define model\n",
    "model = RidgeCV(alphas=[0.001, 0.01, 1, 10], cv=cv) # alphas here refer to lambda values to try\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % model.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # or\n",
    "# model = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8, n_jobs=-1)\n",
    "# # fit model\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make predictions and evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the trained model to make predictions in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate performance in the test set by comparing predictions to the label data.We can then evaluate performance in the test set by comparing predictions to the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get R^2 from true and predicted values\n",
    "print('r2: %f' % r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a scatter plot of labeled data against predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip bounds so min value = 0 because we know that treecover cannot be below zero. \n",
    "y_pred[y_pred<0] = 0\n",
    "\n",
    "#scatterplot\n",
    "ax = sns.scatterplot(x = y_pred, y = y_test)\n",
    "ax.set(xlabel='Predicted', ylabel='Observed')\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the spatial distribution of label (i.e. observed) values with predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge lat/lons with test and predicted values\n",
    "map_plot = pd.DataFrame(plotting_coords)\n",
    "map_plot['predicted'] = y_pred\n",
    "map_plot['observed'] = y_test\n",
    "map_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "\n",
    "vmax = max(y_test.max(), y_pred.max())\n",
    "vmin = min(y_test.min(), y_pred.min())\n",
    "\n",
    "# plot observed\n",
    "map_plot.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"Lon\",\n",
    "    y=\"Lat\",\n",
    "    c=\"observed\",\n",
    "    colorbar=False,\n",
    "    alpha=0.5,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Observed\")\n",
    "\n",
    "# plot predicted\n",
    "map_plot.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"Lon\",\n",
    "    y=\"Lat\",\n",
    "    c=\"predicted\",\n",
    "    colorbar=False,\n",
    "    alpha=0.5,\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('nudge_up')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "142c193bf060635deeb675579e1db6ca9d9f29c8a237f64acc594fb64723fb97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
