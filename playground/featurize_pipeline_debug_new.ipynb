{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a4efef4-fe2f-4da2-bc72-374966fea981",
   "metadata": {},
   "source": [
    "# README for DEBUGGING\n",
    "\n",
    "Note: By default, solution 1 below is applied in this branch.\n",
    "\n",
    "### For the implementation with errors\n",
    "change the following in `stacs.py`:\n",
    "- In `fetch_stac_items()` - use unmodified image shapes for point matching on line 222 instead of line 224\n",
    "- In `CustomDataSet` -  use the original stacstack implementation beginning at line 406 (searches in the output image crs)\n",
    "\n",
    "### For solution 1 \n",
    "Only assigning images that cover our point with both their shape and `proj:bbox` shape:\n",
    "- In `fetch_stac_items()` - use `_get_trimmed_stac_shapes_gdf()` on line 224 instead of line 222\n",
    "\n",
    "### For solution 2 - \n",
    "using stackstac in the latlong 4326 crs:\n",
    "- Revert `fetch_stac_items()` to use image shapes directly\n",
    "- In `CustomDataSet` use the new stacstack implementation beginning at line 423 (searches in latlon 4326 crs)\n",
    "\n",
    "\n",
    "Note: The resolution parameter in stackstac become a bit complicated in this solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf41322",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a082089-9f4a-486e-a3a5-4b41cb161c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory\n",
    "import os\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to the root of the project\n",
    "os.chdir(\"/home/jovyan/ds_nudge_up/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc1c53-a346-4a42-8285-105df051c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path += [\"../\"]\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc11b6-27f9-4246-87e8-c3a2981a69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src.mosaiks.utils as utl\n",
    "\n",
    "rasterio_config = utl.load_yaml_config(\"rasterioc_config.yaml\")\n",
    "os.environ.update(rasterio_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce518c-4b7d-4af1-af07-1788fac82156",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `pip install` MOSAIKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556c86b-f2a5-48c6-bd21-ef3f92317059",
   "metadata": {},
   "source": [
    "From local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77ee2f-298a-4cb0-876d-1acc2c9dc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5c520-3a16-4a1d-8118-3f8dc0192a32",
   "metadata": {},
   "source": [
    "From GitHub package:\n",
    "\n",
    "ðŸš¨ðŸš¨ **Make sure you update github token in the secrets file** ðŸš¨ðŸš¨ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbef195-1409-45f8-b940-1b79ebf162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secrets = utl.load_yaml_config(\"../config/secrets.yml\")\n",
    "# GITHUB_TOKEN = secrets[\"GITHUB_TOKEN\"]\n",
    "# mosaiks_package_link = f\"git+https://{GITHUB_TOKEN}@github.com/IDinsight/ds_nudge_up@as-package\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aad8c-aa40-463b-b8bd-775321cc9393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall mosaiks -y\n",
    "# !pip install {mosaiks_package_link} --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237a534-1c05-4455-8685-f1aef6686f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Dask Cluster and Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72152acf-a27d-416e-84ce-893fe3651598",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1438e38-662f-4edc-a137-905895ef5f59",
   "metadata": {},
   "source": [
    "4 workers with 4 threads each seem to work best. A lot of time a thread is waiting on data to load so CPU is underutilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55239c8c-df3e-45ec-ba3d-3dda44ab41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4, processes=True, threads_per_worker=4, silence_logs=logging.ERROR\n",
    ")\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810aae99-6eba-4c02-987d-5261696fc74d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gateway cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f8fa2-932f-4dfc-ac81-b0f1ed39cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_gateway import Gateway\n",
    "# import dask_gateway\n",
    "# from dask.distributed import PipInstall\n",
    "\n",
    "# gateway = Gateway()\n",
    "# options = gateway.cluster_options()\n",
    "# options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d33283-1345-4864-9058-15cd5dcf1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import PipInstall\n",
    "\n",
    "# cluster = gateway.new_cluster(options)\n",
    "# client = cluster.get_client()\n",
    "# print(cluster.dashboard_link)\n",
    "\n",
    "# plugin = PipInstall(packages=[mosaiks_package_link], pip_options=[\"--upgrade\"], restart=False)\n",
    "# client.register_worker_plugin(plugin)\n",
    "\n",
    "# cluster.scale(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728e259-2140-4a39-b241-c3e64d97bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb924c-7f6c-429f-93fb-2a1eabe394e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f69a5-5ed3-4938-bc83-9bfe618418dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosaiks.featurize import *\n",
    "\n",
    "from dask import delayed\n",
    "from dask.distributed import as_completed\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb14555-bbba-4e63-a157-4a247e1af77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurization_params = utl.load_yaml_config(\"featurisation.yaml\")\n",
    "satellite_config = utl.load_yaml_config(\"satellite_config.yaml\")\n",
    "satellite_config = satellite_config[\n",
    "    featurization_params[\"satellite_search_params\"][\"satellite_name\"]\n",
    "]\n",
    "data_sources = utl.load_yaml_config(\"data_catalog.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df2287-cc11-43b0-b20b-feef6f4d15ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load point coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66cbd3-eee6-4634-b656-8088e66aeb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = utl.load_points_gdf(**data_sources[\"request_points_centroids\"])\n",
    "\n",
    "focus_states_id_dict = {\n",
    "    20: \"jharkhand\",\n",
    "    22: \"chhattisgarh\",\n",
    "    8: \"rajasthan\",\n",
    "    23: \"madhya pradesh\",\n",
    "    18: \"assam\",\n",
    "    16: \"tripura\",\n",
    "}\n",
    "focus_states_filter = points_gdf[\"pc11_s_id\"].isin(focus_states_id_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf1e80-dab6-46da-9ddc-aef0d6c49b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf_focus = points_gdf[focus_states_filter]\n",
    "points_gdf_focus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00d330-0caa-4c9b-8b71-a4fc5debadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf_focus[\"shrid\"].drop_duplicates(keep=\"first\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = points_gdf_focus.sample(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb467c22-2812-473d-ad0e-ffd26f75dc13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fetch image stac refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b097a3-490c-48e4-85e9-24a24cd95c00",
   "metadata": {},
   "source": [
    "`fetch_image_refs` now returns a dask dataframe and is not yet computed. So it finishes quite quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119647d-9445-4b79-9047-e6f0fcd1c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "points_gdf_with_stac = fetch_image_refs(\n",
    "    points_gdf_focus, \n",
    "    featurization_params['dask']['n_partitions'],\n",
    "    featurization_params['satellite_search_params']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3464f-1c3e-4b57-96af-2a1736b79676",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define delayed objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc8b02-7c2e-4767-bc19-ec31ff86e9c8",
   "metadata": {},
   "source": [
    "We use the `delayed` decorator to turn our function into a delayed function. This means it will not run immediately when called but instead return a delayed object that can be run later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0da56-abb4-4aff-8639-a58a42b34fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def partition_run(df, satellite_config, featurization_params, model, device):\n",
    "\n",
    "    data_loader = create_data_loader(\n",
    "        df, satellite_config, featurization_params[\"batch_size\"]\n",
    "    )\n",
    "    X_features = create_features(\n",
    "        data_loader,\n",
    "        featurization_params[\"num_features\"],\n",
    "        len(df),\n",
    "        model,\n",
    "        device,\n",
    "        satellite_config[\"min_image_edge\"],\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(X_features, index=df.index.copy())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a177bfc-4be1-4954-99c2-3dd3648be50c",
   "metadata": {},
   "source": [
    "We want to convert our dask dataframe into \"delayed\" objects. Each partition is now a delayed pandas dataframe and can be passed to our delayed function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0e28d-d900-4c40-a90b-18bcab88eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = points_gdf_with_stac.to_delayed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856d586-8a84-4dde-b0ff-55ab39e11cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCF(\n",
    "    featurization_params[\"num_features\"],\n",
    "    featurization_params[\"kernel_size\"],\n",
    "    len(satellite_config[\"bands\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97812f-8294-4a78-88d6-97ae5375091f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a7e10-6981-42e6-a5da-ca70b3164399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import stackstac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368bec0-b70d-4d49-9af6-bb0015ee59e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run for the problematic partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a730a-9ceb-41af-933c-e44e8fec7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_44 = partitions[44].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be83d4-6411-4491-a463-0717df745300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loop below or run for selective indices using...\n",
    "# i = 500015 # 500045 fails, 500015 works\n",
    "# row = p_44.loc[i]\n",
    "\n",
    "failing_IDs = []\n",
    "for i, row in p_44.iterrows():\n",
    "\n",
    "    stac_item = row[\"stac_item\"]\n",
    "    lat = row[\"Lat\"]\n",
    "    lon = row[\"Lon\"]\n",
    "    buffer = 1200\n",
    "\n",
    "    # convert point latlons to image crs and create buffer using meters\n",
    "    stac_crs = stac_item.properties[\"proj:epsg\"]\n",
    "    proj_latlon_to_stac = pyproj.Transformer.from_crs(4326, stac_crs, always_xy=True)\n",
    "    x_utm, y_utm = proj_latlon_to_stac.transform(lon, lat)\n",
    "\n",
    "    x_min, x_max = x_utm - buffer, x_utm + buffer\n",
    "    y_min, y_max = y_utm - buffer, y_utm + buffer\n",
    "\n",
    "    # convert buffer bounds back to latlons\n",
    "    proj_stac_to_latlon = pyproj.Transformer.from_crs(stac_crs, 4326, always_xy=True)\n",
    "    x_min, y_min = proj_stac_to_latlon.transform(x_min, y_min)\n",
    "    x_max, y_max = proj_stac_to_latlon.transform(x_max, y_max)\n",
    "\n",
    "    xarray = stackstac.stack(\n",
    "        stac_item,\n",
    "        assets=satellite_config[\"bands\"],\n",
    "        epsg=4326,\n",
    "        resolution=0.00027,  # satellite_config[\"resolution\"],\n",
    "        bounds_latlon=[x_min, y_min, x_max, y_max],\n",
    "        # rescale=False,\n",
    "        dtype=np.uint8,\n",
    "        fill_value=0,\n",
    "        # snap_bounds=False\n",
    "    )\n",
    "\n",
    "    # if time dimension is 0 it means the returned xarray is unusable\n",
    "    time_dim = xarray.shape[0]\n",
    "    p_44.loc[i, \"xarray_time_dim\"] = time_dim\n",
    "\n",
    "    # Check where the point sits inside the image\n",
    "\n",
    "    # 1. STAC geometry (in same projection as STAC image and crop)\n",
    "    stac_shape = shapely.geometry.shape(stac_item.geometry)\n",
    "    stac_shape = gpd.GeoSeries(stac_shape).set_crs(\"EPSG:4326\").geometry[0] # .to_crs(stac_crs)\n",
    "\n",
    "    # 2. Use the STAC proj:bbox property to make a shape\n",
    "    x_min_p, y_min_p, x_max_p, y_max_p = stac_item.properties[\"proj:bbox\"]\n",
    "    image_bbox = shapely.geometry.Polygon(\n",
    "        [[x_min_p, y_min_p], [x_min_p, y_max_p], [x_max_p, y_max_p], [x_max_p, y_min_p]]\n",
    "    )\n",
    "    image_bbox = gpd.GeoSeries(image_bbox).set_crs(stac_crs).to_crs(\"EPSG:4326\").geometry[0]\n",
    "\n",
    "    # 3. Convert the crop square to a shape\n",
    "    crop_square = shapely.geometry.Polygon(\n",
    "        [[x_min, y_min], [x_min, y_max], [x_max, y_max], [x_max, y_min]]\n",
    "    )\n",
    "\n",
    "    # Store whether the crop sits within this bounding box/STAC geometry\n",
    "    p_44.loc[i, \"intersects_bbox\"] = crop_square.intersects(image_bbox)\n",
    "    p_44.loc[i, \"intersects_geometry\"] = crop_square.intersects(stac_shape)\n",
    "    # p_44.loc[i, \"xarray_sum\"] = np.array(xarray).sum()\n",
    "\n",
    "#     if p_44.loc[i, \"xarray_sum\"] == 0: #p_44.loc[i, \"xarray_time_dim\"] == 0:\n",
    "#         failing_IDs.append(i)\n",
    "\n",
    "#         print(i)\n",
    "#         print(\"Intersects bbox?\", p_44.loc[i, \"intersects_bbox\"])\n",
    "#         print(\"Intersects STAC geometry?\", p_44.loc[i, \"intersects_geometry\"])\n",
    "\n",
    "#         # plot all shapes\n",
    "#         shapes_gdf = gpd.GeoDataFrame(\n",
    "#             {\"item\":[\"stac_shape\", \"image_bbox\", \"crop\"]},\n",
    "#             geometry=[stac_shape, image_bbox, crop_square]\n",
    "#         ).set_crs(\"EPSG:4326\")\n",
    "\n",
    "#         shapes_gdf.plot(column=\"item\", legend=True, alpha=0.6, figsize=(4,4))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eade01-c612-40f8-82c6-7f2535d8ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many of the failing points sit inside/outside STAC geometry\n",
    "crosstab_geom = pd.crosstab(\n",
    "    index=p_44[\"xarray_time_dim\"], columns=p_44[\"intersects_geometry\"]\n",
    ")\n",
    "crosstab_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c94ddc-7c66-4eee-8c5d-ba2f7d4f39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many of the failing points sit inside/outside bbox\n",
    "crosstab_bbox = pd.crosstab(\n",
    "    index=p_44[\"xarray_time_dim\"], columns=p_44[\"intersects_bbox\"]\n",
    ")\n",
    "crosstab_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511c8b8-829f-497c-8999-b1da20572850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_44[~p_44[\"xarray_sum\"]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578514d6-d755-4f55-b948-2dd3a7f66c6e",
   "metadata": {},
   "source": [
    "So every point that fails sits within the STAC geometry (by definition, since only STAC items that had points inside were fetched based on their given geometry) but outside the \"proj:bbox\" shape - this must be a data/coding issue on the database end since the bbox should always match the geometry parameter perfectly but sometimes it does not. \n",
    "\n",
    "Two solutions:\n",
    "1. Trim the geometries to within only the area within the bbox (implemented through the `_get_trimmed_stac_shapes_gdf` function in `stacs.py`) when trying to select which STAC item(s) to return for each point. This results in a different STAC item being returned as least cloudy, for example, than the problematic one.\n",
    "2. Catch xarrays with a 0 time dimension as errors and return None for the points that suffer from this issue. Not preferable as we lose datapoints for no real reason (usually there are other valid images that could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71866155-9072-4f21-8d6a-781d378b6d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d470e-3bd8-4fdf-aa7f-cb1521f67b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trial run\n",
    "\n",
    "The cell below will only run it for 8 of the partitions. That seems to be about how many we can do in parallel on a local cluster. We may be able to do more on a Gateway Cluster once that is working.\n",
    "\n",
    "There are also better schemes. For example, kick off another partitions whenever one finishes. That might be a better use of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd0262-588b-46c8-8cb8-efc7b54ac844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "p = partitions[i]\n",
    "f = partition_run(p, satellite_config, featurization_params, model, 'cuda', dask_key_name=f'run_{i}')\n",
    "df_future = client.compute(f)\n",
    "for f in as_completed([df_future]):\n",
    "    df = f.result()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8779ebe-592d-4702-beaa-2c5788d5bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dba8a3-fb91-4fd1-9b29-e3a990387b37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Full run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442d12d-4c13-4758-84b1-3c5b3f1e02dc",
   "metadata": {},
   "source": [
    "This is going to create 200 dataframes - one for each partition. If any fail, we can always just rerun that single component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657daae-a2fc-4249-b0df-48adca87575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# N_PARTITIONS = len(partitions)\n",
    "# N_PER_RUN = 8\n",
    "# START_IDX = 44\n",
    "# str_column_names = [str(i) for i in range(featurization_params['num_features'])]\n",
    "\n",
    "# p_ids = np.arange(START_IDX, N_PARTITIONS + N_PER_RUN, N_PER_RUN)\n",
    "\n",
    "# for p_start_id, p_end_id in zip(p_ids[:-1], p_ids[1:]):\n",
    "#     now = datetime.now().strftime(\"%d-%b %H:%M:%S\")\n",
    "#     print(f\"{now} Running batch: \", p_start_id, \"to\", p_end_id-1)\n",
    "\n",
    "#     delayed_dfs = []\n",
    "#     for i, p in enumerate(partitions[p_start_id:p_end_id]):\n",
    "#         f = partition_run(p, satellite_config, featurization_params, model,\n",
    "#                           featurization_params['device'], dask_key_name=f'features_{p_start_id + i}')\n",
    "#         delayed_dfs.append(f)\n",
    "#     futures_dfs = client.compute(delayed_dfs)\n",
    "\n",
    "#     for f in as_completed(futures_dfs):\n",
    "#         try:\n",
    "#             df = f.result()\n",
    "#             df.columns = str_column_names\n",
    "#             df.to_parquet(f'data/df_{f.key}.parquet.gzip', compression='gzip')\n",
    "#         except Exception as e:\n",
    "#             print(f\"Partition {f.key} failed. Error:\", e)\n",
    "\n",
    "#     client.restart()\n",
    "#     sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cc39b-ac6d-428d-a5e8-37e29e5a1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in as_completed(futures_dfs[-3:]):\n",
    "#     df = f.result()\n",
    "#     df.columns = str_column_names\n",
    "#     df.to_parquet(f'data/df_{f.key}.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4863d-9579-4d88-bd7c-5461dd5b08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72982cdc-6475-44cc-9281-e15ba90df338",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Re-run failed partitions\n",
    "\n",
    "Use this to just run partitions that failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72476f-804d-4892-82d3-5fe5c0c27006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# FAILED_IDX = [44]\n",
    "\n",
    "# delayed_dfs = []\n",
    "# for i in FAILED_IDX:\n",
    "#     p = partitions[i]\n",
    "#     f = partition_run(p, satellite_config, featurization_params, model,\n",
    "#                       featurization_params['device'], dask_key_name=f'features_{i}')\n",
    "#     delayed_dfs.append(f)\n",
    "#     futures_dfs = client.compute(delayed_dfs)\n",
    "\n",
    "#     for f in as_completed(futures_dfs):\n",
    "#         f.result().to_csv(f'data/df_{f.key}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346620c-362e-49ad-acb4-d25eebf6166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b7689-36ef-4713-ba0f-b37c34103b5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load checkpoint files and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a823bd5-c513-42b4-9dd4-8f91a6aa5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"./data\"\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "# Select only CSV files from the folder\n",
    "parquet_files = sorted([file for file in all_files if file.endswith(\".gzip\")])\n",
    "parquet_files = parquet_files[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2c7f9-1b17-47a0-a286-bca348b78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(parquet_files).to_csv(\"./data/file_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255654f2-b8ca-433f-a738-4d6ab62dadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for filename in parquet_files:\n",
    "\n",
    "    df = pd.read_parquet(\"./data/\" + filename)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, axis=0)\n",
    "print(\"Dataset size in memory (MB):\", combined_df.memory_usage().sum() / 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89d980-1c5b-4e03-9480-124eabce7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545bc1f-52e8-4260-a452-b24e60561f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0d726-9d86-4fdb-ba58-8d4e0adbae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_parquet(\n",
    "    \"centroid_features_landsat_TEMP.parquet.gzip\", compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cee2c3-0f95-4889-8ea0-a9dbc7868a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"df_features_198.parquet.gzip\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
