{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8]}\n",
    "# df = dd.from_pandas(pd.DataFrame(data=d), npartitions=2)\n",
    "# dd.to_parquet(df=df,\n",
    "#               path='abfs://CONTAINER/FILE.parquet'\n",
    "#               storage_options={'account_name': 'ACCOUNT_NAME',\n",
    "#                                'account_key': 'ACCOUNT_KEY'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUEUED RUN - OLD CODE #### \n",
    "\n",
    "# def get_partitions_generator(\n",
    "#     points_gdf: gpd.GeoDataFrame,\n",
    "#     chunksize: int,\n",
    "#     sort_points_by_hilbert_distance: bool = False,\n",
    "# ) -> Generator[gpd.GeoDataFrame, None, None]:\n",
    "#     \"\"\"\n",
    "#     Given a GeoDataFrame, this function creates a generator that returns chunksize\n",
    "#     number of rows per iteration.\n",
    "\n",
    "#     To be used for submitting Dask Futures.\n",
    "\n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     points_gdf : GeoDataFrame of points to be featurized.\n",
    "#     chunksize : Number of points to be featurized per iteration.\n",
    "#     sort_points_by_hilbert_distance : Whether to sort the points by their Hilbert distance.\n",
    "\n",
    "#     Returns\n",
    "#     --------\n",
    "#     Generator\n",
    "#     \"\"\"\n",
    "\n",
    "#     if sort_points_by_hilbert_distance:\n",
    "#         points_gdf = _sort_points_by_hilbert_distance(points_gdf)\n",
    "\n",
    "#     num_chunks = math.ceil(len(points_gdf) / chunksize)\n",
    "\n",
    "#     logging.info(\n",
    "#         f\"Distributing {len(points_gdf)} points across {chunksize}-point partitions \"\n",
    "#         f\"results in {num_chunks} partitions.\"\n",
    "#     )\n",
    "\n",
    "#     for i in range(num_chunks):\n",
    "#         yield points_gdf.iloc[i * chunksize : (i + 1) * chunksize]\n",
    "\n",
    "\n",
    "# def run_queued_futures_pipeline(\n",
    "#     points_gdf: gpd.GeoDataFrame,\n",
    "#     client: Client,\n",
    "#     model: nn.Module,\n",
    "#     satellite_name: str,\n",
    "#     image_resolution: int,\n",
    "#     image_dtype: str,\n",
    "#     image_bands: list[str],\n",
    "#     image_width: int,\n",
    "#     min_image_edge: int,\n",
    "#     sort_points_by_hilbert_distance: bool,\n",
    "#     seasonal: bool,\n",
    "#     year: int,\n",
    "#     search_start: str,\n",
    "#     search_end: str,\n",
    "#     image_composite_method: str,\n",
    "#     stac_api_name: str,\n",
    "#     num_features: int,\n",
    "#     device: str,\n",
    "#     col_names: list,\n",
    "#     n_concurrent_tasks: int,\n",
    "#     chunksize: int,\n",
    "#     output_folderpath: str,\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     For a given GeoDataFrame of coordinate points, this function partitions it\n",
    "#     and submit each partition to be processed as a Future on the Dask client.\n",
    "\n",
    "#     Initially, only as many partitions are submitted as there are threads. As each\n",
    "#     partition is completed, another partition is submitted to the client.\n",
    "\n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     points_gdf : GeoDataFrame of points to be featurized.\n",
    "#     client : Dask client.\n",
    "#     model : PyTorch model to be used for featurization.\n",
    "#     satellite_name : Name of the satellite to be used for featurization.\n",
    "#     image_resolution : Resolution of the image to be generated.\n",
    "#     image_dtype : Data type of the image to be generated.\n",
    "#     image_bands : List of bands to be used for generating the image.\n",
    "#     image_width : Desired width of the image to be fetched (in meters).\n",
    "#     min_image_edge : Minimum edge length of the image to be generated.\n",
    "#     sort_points_by_hilbert_distance : Whether to sort the points by their Hilbert distance.\n",
    "#     seasonal : Whether to use seasonal imagery.\n",
    "#     year : Year of imagery to be used.\n",
    "#     search_start : Start date of imagery to be used.\n",
    "#     search_end : End date of imagery to be used.\n",
    "#     image_composite_method : Mosaic composite to be used.\n",
    "#     stac_api_name : Name of the STAC API to be used.\n",
    "#     num_features : Number of features to be extracted from the model.\n",
    "#     device : Device to be used for featurization.\n",
    "#     col_names : List of column names to be used for saving the features.\n",
    "#     n_concurrent_tasks : Number of concurrent partitions to be submitted to the client.\n",
    "#     chunksize : Number of points to be featurized per partition.\n",
    "#     output_folderpath : Path to folder where features will be saved.\n",
    "\n",
    "#     Returns\n",
    "#     --------\n",
    "#     None\n",
    "#     \"\"\"\n",
    "\n",
    "#     # make generator for spliting up the data into partitions\n",
    "#     partitions = get_partitions_generator(\n",
    "#         points_gdf,\n",
    "#         chunksize,\n",
    "#         sort_points_by_hilbert_distance,\n",
    "#     )\n",
    "\n",
    "#     # if n_concurrent_tasks is not specified, use all available threads\n",
    "#     if n_concurrent_tasks is None:\n",
    "#         n_concurrent_tasks = sum(client.nthreads().values())\n",
    "#     # if there are less partitions to run than concurrent tasks, run all partitions\n",
    "#     n_concurrent_tasks = min(len(partitions), n_concurrent_tasks)\n",
    "\n",
    "#     # kickoff \"n_concurrent_tasks\" number of tasks. Each of these will be replaced by a new\n",
    "#     # task upon completion.\n",
    "#     now = datetime.now().strftime(\"%d-%b %H:%M:%S\")\n",
    "#     logging.info(f\"{now} Trying to kick off initial {n_concurrent_tasks} partitions...\")\n",
    "#     initial_futures_list = []\n",
    "#     for i in range(n_concurrent_tasks):\n",
    "#         try:\n",
    "#             partition = next(partitions)\n",
    "#         except StopIteration:\n",
    "#             logging.info(\n",
    "#                 f\"There are less partitions than processors. All {i} partitions running.\"\n",
    "#             )\n",
    "#             wait(initial_futures_list)\n",
    "#             break\n",
    "\n",
    "#         future = client.submit(\n",
    "#             run_pipeline,\n",
    "#             points_gdf=partition,\n",
    "#             model=model,\n",
    "#             satellite_name=satellite_name,\n",
    "#             image_resolution=image_resolution,\n",
    "#             image_dtype=image_dtype,\n",
    "#             image_bands=image_bands,\n",
    "#             image_width=image_width,\n",
    "#             min_image_edge=min_image_edge,\n",
    "#             seasonal=seasonal,\n",
    "#             year=year,\n",
    "#             search_start=search_start,\n",
    "#             search_end=search_end,\n",
    "#             image_composite_method=image_composite_method,\n",
    "#             stac_api_name=stac_api_name,\n",
    "#             num_features=num_features,\n",
    "#             device=device,\n",
    "#             col_names=col_names,\n",
    "#             output_folderpath=output_folderpath,\n",
    "#             save_filename=f\"df_{str(i).zfill(3)}.parquet.gzip\",\n",
    "#             return_df=False,\n",
    "#         )\n",
    "#         initial_futures_list.append(future)\n",
    "\n",
    "#     # get generator that returns futures as they are completed\n",
    "#     as_completed_generator = as_completed(initial_futures_list)\n",
    "\n",
    "#     # only run each remaining partitions once a previous task has completed\n",
    "#     for partition in partitions:\n",
    "#         i += 1\n",
    "#         completed_future = next(as_completed_generator)\n",
    "#         logging.info(f\"Adding partition {i}\")\n",
    "\n",
    "#         new_future = client.submit(\n",
    "#             run_pipeline,\n",
    "#             points_gdf=partition,\n",
    "#             model=model,\n",
    "#             satellite_name=satellite_name,\n",
    "#             image_resolution=image_resolution,\n",
    "#             image_dtype=image_dtype,\n",
    "#             image_bands=image_bands,\n",
    "#             image_width=image_width,\n",
    "#             min_image_edge=min_image_edge,\n",
    "#             seasonal=seasonal,\n",
    "#             year=year,\n",
    "#             search_start=search_start,\n",
    "#             search_end=search_end,\n",
    "#             image_composite_method=image_composite_method,\n",
    "#             stac_api_name=stac_api_name,\n",
    "#             num_features=num_features,\n",
    "#             device=device,\n",
    "#             col_names=col_names,\n",
    "#             output_folderpath=output_folderpath,\n",
    "#             save_filename=f\"df_{str(i).zfill(3)}.parquet.gzip\",\n",
    "#             return_df=False,\n",
    "#         )\n",
    "#         as_completed_generator.add(new_future)\n",
    "\n",
    "#     # wait for all futures to process\n",
    "#     for completed_future in as_completed_generator:\n",
    "#         pass\n",
    "\n",
    "#     now = datetime.now().strftime(\"%d-%b %H:%M:%S\")\n",
    "#     logging.info(f\"{now} Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BATCHED RUNNING. REQUIRES POINTS_GDF_WITH_STAC to start ####\n",
    "\n",
    "# def run_partitions(\n",
    "#     partitions: list,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "#     mosaiks_folder_path: str = None,\n",
    "#     partition_ids: list = None,\n",
    "# ) -> list:\n",
    "#     \"\"\"Run partitions in batches of n_per_run and save the result for each partition\n",
    "#     to a parquet file. If a partition fails to be featurized, the partition ID is added\n",
    "#     to a list and returned at the end of the run.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partitions : List of dataframes.\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "#     partition_ids : List of partition IDs corresponding to each partition in `partitions`.\n",
    "#         If None, the partition IDs will be inferred from the order of the\n",
    "#         partitions in the list. Default is None.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition IDs that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     n_per_run = featurization_config[\"dask\"][\"n_per_run\"]\n",
    "#     n_partitions = len(partitions)\n",
    "#     logging.info(f\"Running {n_partitions} partitions...\")\n",
    "\n",
    "#     if n_partitions < n_per_run:\n",
    "#         logging.info(\n",
    "#             f\"n_partitions is smaller than n_per_run. Running all {n_partitions} partitions.\"\n",
    "#         )\n",
    "#         n_per_run = n_partitions\n",
    "\n",
    "#     if partition_ids is None:\n",
    "#         partition_ids = list(range(n_partitions))\n",
    "\n",
    "#     mosaiks_column_names = [\n",
    "#         f\"mosaiks_{i}\" for i in range(featurization_config[\"model\"][\"num_features\"])\n",
    "#     ]\n",
    "\n",
    "#     failed_ids = []\n",
    "#     checkpoint_indices = np.arange(0, n_partitions + n_per_run, n_per_run)\n",
    "#     for p_start_id, p_end_id in zip(checkpoint_indices[:-1], checkpoint_indices[1:]):\n",
    "\n",
    "#         now = datetime.now().strftime(\"%d-%b %H:%M:%S\")\n",
    "#         logging.info(f\"{now} Running batch: {p_start_id} to {p_end_id - 1}\")\n",
    "\n",
    "#         batch_indices = list(range(p_start_id, p_end_id))\n",
    "#         batch_p_ids = [partition_ids[i] for i in batch_indices]\n",
    "#         batch_partitions = [partitions[i] for i in batch_indices]\n",
    "\n",
    "#         failed_ids += run_batch(\n",
    "#             partitions=batch_partitions,\n",
    "#             partition_ids=batch_p_ids,\n",
    "#             satellite_config=satellite_config,\n",
    "#             featurization_config=featurization_config,\n",
    "#             mosaiks_column_names=mosaiks_column_names,\n",
    "#             model=model,\n",
    "#             client=client,\n",
    "#             mosaiks_folder_path=mosaiks_folder_path,\n",
    "#         )\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def run_batch(\n",
    "#     partitions: list,\n",
    "#     partition_ids: list,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     mosaiks_column_names: list,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "#     mosaiks_folder_path: str,\n",
    "# ) -> list:\n",
    "#     \"\"\"\n",
    "#     Run a batch of partitions and save the result for each partition to a parquet file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partitions :List of dataframes to process.\n",
    "#     partition_ids : List containing IDs corresponding to the partitions passed (to be used\n",
    "#         for naming saved files and reference in case of failure).\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition labels that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     failed_ids = []\n",
    "#     delayed_dfs = []\n",
    "\n",
    "#     # collect futures\n",
    "#     for p_id, p in zip(partition_ids, partitions):\n",
    "#         str_id = str(p_id).zfill(3)  # makes 1 into '001'\n",
    "\n",
    "#         f = delayed_partition_run(\n",
    "#             df=p,\n",
    "#             satellite_config=satellite_config,\n",
    "#             featurization_config=featurization_config,\n",
    "#             mosaiks_column_names=mosaiks_column_names,\n",
    "#             model=model,\n",
    "#             dask_key_name=f\"features_{str_id}\",\n",
    "#         )\n",
    "#         delayed_dfs.append(f)\n",
    "\n",
    "#     # delayed -> futures -> collected results\n",
    "#     futures_dfs = client.compute(delayed_dfs)\n",
    "#     failed_ids = collect_results(\n",
    "#         futures_dfs=futures_dfs, mosaiks_folder_path=mosaiks_folder_path\n",
    "#     )\n",
    "\n",
    "#     # prep for next run\n",
    "#     client.restart()\n",
    "#     sleep(5)\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def collect_results(futures_dfs: list, mosaiks_folder_path: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Save computed dataframes to parquet files. If a partition fails to be featurized,\n",
    "#     the partition ID is added to a list.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     futures_dfs : List of futures containing the computed dataframes.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition IDs that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     failed_ids = []\n",
    "#     for f in as_completed(futures_dfs):\n",
    "#         try:\n",
    "#             df = f.result()\n",
    "#             utl.save_dataframe(\n",
    "#                 df=df, file_path=f\"{mosaiks_folder_path}/df_{f.key}.parquet.gzip\"\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             f_key = f.key\n",
    "#             partition_id = int(f_key.split(\"features_\")[1])\n",
    "#             logging.info(f\"Partition {partition_id} failed. Error:\", e)\n",
    "#             failed_ids.append(partition_id)\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def run_single_partition(\n",
    "#     partition: pd.DataFrame,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Run featurization for a single partition. For testing.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partition : Dataframe containing the data to featurize.\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : Dataframe containing the featurized data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     mosaiks_column_names = [\n",
    "#         f\"mosaiks_{i}\" for i in range(featurization_config[\"model\"][\"num_features\"])\n",
    "#     ]\n",
    "\n",
    "#     f = delayed_partition_run(\n",
    "#         df=partition,\n",
    "#         satellite_config=satellite_config,\n",
    "#         featurization_config=featurization_config,\n",
    "#         mosaiks_column_names=mosaiks_column_names,\n",
    "#         model=model,\n",
    "#         dask_key_name=\"single_run\",\n",
    "#     )\n",
    "\n",
    "#     df_future = client.compute(f)\n",
    "#     for f in as_completed([df_future]):\n",
    "#         df = f.result()\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# @delayed\n",
    "# def delayed_partition_run(\n",
    "#     df: pd.DataFrame,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     mosaiks_column_names: list,\n",
    "#     model: nn.Module,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Run featurization for a single partition.\"\"\"\n",
    "\n",
    "#     data_loader = create_data_loader(\n",
    "#         points_gdf_with_stac=df,\n",
    "#         satellite_params=satellite_config,\n",
    "#         batch_size=featurization_config[\"model\"][\"batch_size\"],\n",
    "#     )\n",
    "\n",
    "#     X_features = create_features(\n",
    "#         dataloader=data_loader,\n",
    "#         n_features=featurization_config[\"model\"][\"num_features\"],\n",
    "#         n_points=len(df),\n",
    "#         model=model,\n",
    "#         device=featurization_config[\"model\"][\"device\"],\n",
    "#         min_image_edge=satellite_config[\"min_image_edge\"],\n",
    "#     )\n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         data=X_features, index=df.index.copy(), columns=mosaiks_column_names\n",
    "#     )\n",
    "\n",
    "#     return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
