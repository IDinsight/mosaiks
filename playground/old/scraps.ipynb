{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8]}\n",
    "# df = dd.from_pandas(pd.DataFrame(data=d), npartitions=2)\n",
    "# dd.to_parquet(df=df,\n",
    "#               path='abfs://CONTAINER/FILE.parquet'\n",
    "#               storage_options={'account_name': 'ACCOUNT_NAME',\n",
    "#                                'account_key': 'ACCOUNT_KEY'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_dask_client():\n",
    "\n",
    "        from dask.distributed import PipInstall\n",
    "        from dask_gateway import Gateway\n",
    "\n",
    "        gateway = Gateway()\n",
    "        options = gateway.cluster_options()\n",
    "        options.worker_cores = 4\n",
    "        options.worker_memory = \"8GiB\"\n",
    "\n",
    "        cluster = gateway.new_cluster(options)\n",
    "        logging.info(cluster.dashboard_link)\n",
    "        client = cluster.get_client()\n",
    "\n",
    "        mosaiks_package_link = utl.get_mosaiks_package_link()\n",
    "        plugin = PipInstall(\n",
    "            packages=[mosaiks_package_link], pip_options=[\"--upgrade\"], restart=False\n",
    "        )\n",
    "        client.register_worker_plugin(plugin)\n",
    "\n",
    "        cluster.scale(10)\n",
    "\n",
    "        return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep points in the focus states\n",
    "focus_states_id_dict = {\n",
    "    20: \"jharkhand\",\n",
    "    22: \"chhattisgarh\",\n",
    "    8: \"rajasthan\",\n",
    "    23: \"madhya pradesh\",\n",
    "    18: \"assam\",\n",
    "    16: \"tripura\",\n",
    "}\n",
    "\n",
    "focus_states_filter = request_points_gdf[\"pc11_s_id\"].isin(focus_states_id_dict.keys())\n",
    "points_gdf_focus = request_points_gdf[focus_states_filter]\n",
    "points_gdf_focus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_image = torch.from_numpy(image).float()\n",
    "image = image.astype(np.float)\n",
    "image = minmax_normalize_image(image)\n",
    "torch_image = torch.from_numpy(image)\n",
    "plt.imshow(image[3:0].transpose(1,2,0))\n",
    "plt.savefig(\"TEST.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RASTERIO-only CustomDataSet. Used for speed experiments.\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        points,\n",
    "        items,\n",
    "        buffer,\n",
    "        bands,\n",
    "        resolution,\n",
    "        dtype\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : np.array\n",
    "            Array of points to sample from\n",
    "        items : list\n",
    "            List of STAC items to sample from\n",
    "        buffer : int\n",
    "            Buffer in meters around each point to sample from\n",
    "        bands : list\n",
    "            List of bands to sample\n",
    "        resolution : int\n",
    "            Resolution of the image to sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self.points = points\n",
    "        self.items = items\n",
    "        self.buffer = buffer\n",
    "        self.bands = bands\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of points in the dataset\"\"\"\n",
    "\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the point to get imagery for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_image : torch.Tensor\n",
    "            Image tensor of shape (C, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        lon, lat = self.points[idx]\n",
    "        stac_item = self.items[idx]\n",
    "\n",
    "        if stac_item is None:\n",
    "            print(f\"Skipping {idx}: No STAC item given.\")\n",
    "            return None\n",
    "        else:\n",
    "            # get STAC item URL\n",
    "            stac_item = stac_item.to_dict()\n",
    "\n",
    "            # url = stac_item[\"assets\"][\"visual\"][\"href\"]\n",
    "            url = stac_item[\"assets\"][\"B02\"][\"href\"]\n",
    "\n",
    "            with rasterio.Env():\n",
    "                with rasterio.open(url, \"r\") as f:\n",
    "                    \n",
    "                    crs = f.crs.to_string()\n",
    "                    proj_latlon_to_stac = pyproj.Transformer.from_crs(4326, crs, always_xy=True)\n",
    "                    lon, lat = proj_latlon_to_stac.transform(lon, lat)\n",
    "                    point_geom = shapely.geometry.Point(lon, lat)\n",
    "\n",
    "                    mask_geom = point_geom.buffer(self.buffer).envelope\n",
    "                    mask_geom_dict = shapely.geometry.mapping(mask_geom)\n",
    "\n",
    "                    try:\n",
    "                        image, out_transform = mask(\n",
    "                            f, [mask_geom_dict], crop=True, \n",
    "                        )\n",
    "                    except ValueError as e:\n",
    "                        if \"Input shapes do not overlap raster.\" in str(e):\n",
    "                            return None\n",
    "\n",
    "            try:\n",
    "                # image = image.astype(np.float)\n",
    "                image = image / 255.0\n",
    "                torch_image = torch.from_numpy(image).float()\n",
    "                return torch_image\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {idx}:\", e)\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BATCHED RUNNING. REQUIRES POINTS_GDF_WITH_STAC to start ####\n",
    "\n",
    "# def run_partitions(\n",
    "#     partitions: list,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "#     mosaiks_folder_path: str = None,\n",
    "#     partition_ids: list = None,\n",
    "# ) -> list:\n",
    "#     \"\"\"Run partitions in batches of n_per_run and save the result for each partition\n",
    "#     to a parquet file. If a partition fails to be featurized, the partition ID is added\n",
    "#     to a list and returned at the end of the run.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partitions : List of dataframes.\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "#     partition_ids : List of partition IDs corresponding to each partition in `partitions`.\n",
    "#         If None, the partition IDs will be inferred from the order of the\n",
    "#         partitions in the list. Default is None.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition IDs that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     n_per_run = featurization_config[\"dask\"][\"n_per_run\"]\n",
    "#     n_partitions = len(partitions)\n",
    "#     logging.info(f\"Running {n_partitions} partitions...\")\n",
    "\n",
    "#     if n_partitions < n_per_run:\n",
    "#         logging.info(\n",
    "#             f\"n_partitions is smaller than n_per_run. Running all {n_partitions} partitions.\"\n",
    "#         )\n",
    "#         n_per_run = n_partitions\n",
    "\n",
    "#     if partition_ids is None:\n",
    "#         partition_ids = list(range(n_partitions))\n",
    "\n",
    "#     mosaiks_column_names = [\n",
    "#         f\"mosaiks_{i}\" for i in range(featurization_config[\"model\"][\"num_features\"])\n",
    "#     ]\n",
    "\n",
    "#     failed_ids = []\n",
    "#     checkpoint_indices = np.arange(0, n_partitions + n_per_run, n_per_run)\n",
    "#     for p_start_id, p_end_id in zip(checkpoint_indices[:-1], checkpoint_indices[1:]):\n",
    "\n",
    "#         now = datetime.now().strftime(\"%d-%b %H:%M:%S\")\n",
    "#         logging.info(f\"{now} Running batch: {p_start_id} to {p_end_id - 1}\")\n",
    "\n",
    "#         batch_indices = list(range(p_start_id, p_end_id))\n",
    "#         batch_p_ids = [partition_ids[i] for i in batch_indices]\n",
    "#         batch_partitions = [partitions[i] for i in batch_indices]\n",
    "\n",
    "#         failed_ids += run_batch(\n",
    "#             partitions=batch_partitions,\n",
    "#             partition_ids=batch_p_ids,\n",
    "#             satellite_config=satellite_config,\n",
    "#             featurization_config=featurization_config,\n",
    "#             mosaiks_column_names=mosaiks_column_names,\n",
    "#             model=model,\n",
    "#             client=client,\n",
    "#             mosaiks_folder_path=mosaiks_folder_path,\n",
    "#         )\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def run_batch(\n",
    "#     partitions: list,\n",
    "#     partition_ids: list,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     mosaiks_column_names: list,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "#     mosaiks_folder_path: str,\n",
    "# ) -> list:\n",
    "#     \"\"\"\n",
    "#     Run a batch of partitions and save the result for each partition to a parquet file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partitions :List of dataframes to process.\n",
    "#     partition_ids : List containing IDs corresponding to the partitions passed (to be used\n",
    "#         for naming saved files and reference in case of failure).\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition labels that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     failed_ids = []\n",
    "#     delayed_dfs = []\n",
    "\n",
    "#     # collect futures\n",
    "#     for p_id, p in zip(partition_ids, partitions):\n",
    "#         str_id = str(p_id).zfill(3)  # makes 1 into '001'\n",
    "\n",
    "#         f = delayed_partition_run(\n",
    "#             df=p,\n",
    "#             satellite_config=satellite_config,\n",
    "#             featurization_config=featurization_config,\n",
    "#             mosaiks_column_names=mosaiks_column_names,\n",
    "#             model=model,\n",
    "#             dask_key_name=f\"features_{str_id}\",\n",
    "#         )\n",
    "#         delayed_dfs.append(f)\n",
    "\n",
    "#     # delayed -> futures -> collected results\n",
    "#     futures_dfs = client.compute(delayed_dfs)\n",
    "#     failed_ids = collect_results(\n",
    "#         futures_dfs=futures_dfs, mosaiks_folder_path=mosaiks_folder_path\n",
    "#     )\n",
    "\n",
    "#     # prep for next run\n",
    "#     client.restart()\n",
    "#     sleep(5)\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def collect_results(futures_dfs: list, mosaiks_folder_path: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Save computed dataframes to parquet files. If a partition fails to be featurized,\n",
    "#     the partition ID is added to a list.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     futures_dfs : List of futures containing the computed dataframes.\n",
    "#     mosaiks_folder_path : Path to the folder where the mosaiks features should be saved.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     failed_ids : List of partition IDs that failed to be featurized.\n",
    "#     \"\"\"\n",
    "\n",
    "#     failed_ids = []\n",
    "#     for f in as_completed(futures_dfs):\n",
    "#         try:\n",
    "#             df = f.result()\n",
    "#             utl.save_dataframe(\n",
    "#                 df=df, file_path=f\"{mosaiks_folder_path}/df_{f.key}.parquet.gzip\"\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             f_key = f.key\n",
    "#             partition_id = int(f_key.split(\"features_\")[1])\n",
    "#             logging.info(f\"Partition {partition_id} failed. Error:\", e)\n",
    "#             failed_ids.append(partition_id)\n",
    "\n",
    "#     return failed_ids\n",
    "\n",
    "\n",
    "# def run_single_partition(\n",
    "#     partition: pd.DataFrame,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     model: nn.Module,\n",
    "#     client: Client,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Run featurization for a single partition. For testing.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     partition : Dataframe containing the data to featurize.\n",
    "#     satellite_config : Dictionary containing the satellite configuration.\n",
    "#     featurization_config : Dictionary containing the featurization parameters.\n",
    "#     model : PyTorch random convolutional feature model.\n",
    "#     client : Dask client.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df : Dataframe containing the featurized data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     mosaiks_column_names = [\n",
    "#         f\"mosaiks_{i}\" for i in range(featurization_config[\"model\"][\"num_features\"])\n",
    "#     ]\n",
    "\n",
    "#     f = delayed_partition_run(\n",
    "#         df=partition,\n",
    "#         satellite_config=satellite_config,\n",
    "#         featurization_config=featurization_config,\n",
    "#         mosaiks_column_names=mosaiks_column_names,\n",
    "#         model=model,\n",
    "#         dask_key_name=\"single_run\",\n",
    "#     )\n",
    "\n",
    "#     df_future = client.compute(f)\n",
    "#     for f in as_completed([df_future]):\n",
    "#         df = f.result()\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# @delayed\n",
    "# def delayed_partition_run(\n",
    "#     df: pd.DataFrame,\n",
    "#     satellite_config: dict,\n",
    "#     featurization_config: dict,\n",
    "#     mosaiks_column_names: list,\n",
    "#     model: nn.Module,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Run featurization for a single partition.\"\"\"\n",
    "\n",
    "#     data_loader = create_data_loader(\n",
    "#         points_gdf_with_stac=df,\n",
    "#         satellite_params=satellite_config,\n",
    "#         batch_size=featurization_config[\"model\"][\"batch_size\"],\n",
    "#     )\n",
    "\n",
    "#     X_features = create_features(\n",
    "#         dataloader=data_loader,\n",
    "#         n_features=featurization_config[\"model\"][\"num_features\"],\n",
    "#         n_points=len(df),\n",
    "#         model=model,\n",
    "#         device=featurization_config[\"model\"][\"device\"],\n",
    "#         min_image_edge=satellite_config[\"min_image_edge\"],\n",
    "#     )\n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         data=X_features, index=df.index.copy(), columns=mosaiks_column_names\n",
    "#     )\n",
    "\n",
    "#     return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
